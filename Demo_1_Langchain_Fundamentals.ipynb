{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b268f9",
   "metadata": {},
   "source": [
    "# ðŸ¦œðŸ”— LangChain Fundamentals: Interactive Demo\n",
    "\n",
    "Welcome to this hands-on demonstration!  \n",
    "\n",
    "This notebook provides a comprehensive, interactive demonstration of the **core components that make LangChain powerful for building production-ready AI applications**.  \n",
    "\n",
    "You'll explore how to communicate with AI models, enable them to use tools, and make them work with your own data through practical, real-world examples.\n",
    "\n",
    "## ðŸ“š What This Demo Covers\n",
    "\n",
    "### **1. Prompt Templates**\n",
    "Learn how to communicate with AI models efficiently using reusable and structured prompts that ensure consistent behavior.\n",
    "\n",
    "### **2. Chat Models**\n",
    "These are the *brains* of your applicationâ€”Large Language Models (LLMs) that generate responses, reason through problems, and follow complex instructions.\n",
    "\n",
    "### **3. LCEL (LangChain Expression Language)**\n",
    "Connect multiple components together to build sophisticated workflows and data processing pipelines.  \n",
    "\n",
    "### **4. Structured Output**\n",
    "Receive clean, typed outputs (like JSON or Pydantic models) for seamless integration with your application code.\n",
    "\n",
    "### **5. Tool Calling**\n",
    "Enable AI models to use external toolsâ€”calculators, APIs, databases, or any custom function you provide.\n",
    "\n",
    "### **6. RAG (Retrieval Augmented Generation)**\n",
    "Provide external knowledge sources (PDFs, documents, databases, web pages) so the AI can *answer questions based on your specific data*.\n",
    "\n",
    "---\n",
    "\n",
    "**Each section includes:**\n",
    "- Clear explanations of the concept\n",
    "- Real-world analogies to make complex ideas intuitive\n",
    "- Working code examples you can run and modify\n",
    "- Technical context to understand what's happening under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96616f",
   "metadata": {},
   "source": [
    "## Setup: API Keys\n",
    "\n",
    "**For Local/Jupyter:** Set your OpenAI key in a `.env` file (skip if already in your shell env).\n",
    "\n",
    "**For Google Colab:** Use Colab's secrets manager:\n",
    "1. Click the ðŸ”‘ key icon in the left sidebar\n",
    "2. Add a new secret with name `OPENAI_API_KEY` and your API key as the value\n",
    "3. The code below will automatically detect and use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c4323f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic cell: prints kernel info and tests imports\n",
    "import sys, traceback, os\n",
    "print(\"Notebook kernel executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version.replace('\n",
    "', ' '))\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"sys.path (truncated):\", sys.path[:10])\n",
    "\n",
    "# Test top-level package import\n",
    "try:\n",
    "    import langchain_core\n",
    "    print(\"langchain_core installed at:\", getattr(langchain_core, '__file__', str(langchain_core)))\n",
    "except Exception:\n",
    "    print(\"Failed to import langchain_core:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test specific symbol import\n",
    "try:\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    print(\"ChatPromptTemplate import: OK\")\n",
    "except Exception:\n",
    "    print(\"Failed to import ChatPromptTemplate from langchain_core.prompts:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Also verify local helper module imports that the notebook uses\n",
    "try:\n",
    "    import langchain_prompts\n",
    "    print(\"langchain_prompts loaded from:\", getattr(langchain_prompts, '__file__', str(langchain_prompts)))\n",
    "except Exception:\n",
    "    print(\"Failed to import local module langchain_prompts:\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d5dd3",
   "metadata": {},
   "source": [
    "## 1) Prompt Templates\n",
    "Think of sending professional emails. You probably have templates like:\n",
    "*\"Dear **{Name}**, I'm writing to inform you about **{Topic}**. Please let me know by **{Deadline}**.\"*\n",
    "\n",
    "You don't compose a brand new email from scratch every timeâ€”you use the same structure and just fill in the blanks with different names, topics, and dates. Prompt Templates work the same mannerâ€”they're reusable blueprints where you swap in dynamic variables without rewriting the entire prompt.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `ChatPromptTemplate`: This is our blueprint.\n",
    "* `{user_query}` and `{todays_date}`: These are the **variables** (the blanks we will fill in later).\n",
    "* We use a System Prompt to tell the AI *who* it is (a Date Assistant) before it answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc3238",
   "metadata": {},
   "outputs": [],
   "source": "from langchain_prompts import DATE_ASSISTANT_SYSTEM_PROMPT\nfrom datetime import datetime\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", DATE_ASSISTANT_SYSTEM_PROMPT),\n    (\"human\", \"{user_query}\")\n])\n\nexample_user_query = \"\"\"\nWhat is tomorrow's date?\n\"\"\"\n\nmessages = prompt.format_messages(user_query = example_user_query, \n                                    todays_date = datetime.now().strftime(\"%Y-%m-%d\"))  # Shows message objects ready for a chat model\n\nmessages\n\n\n# Other Message Objects\n    # System messages\n    # Human messages\n    # AIMessage\n    # ToolMessage\n"
  },
  {
   "cell_type": "markdown",
   "id": "c3883f54",
   "metadata": {},
   "source": [
    "## 2) Chat Models (OpenAI)\n",
    "\n",
    "Think of Chat Models as different brands of TVs (Sony, Samsung, LG). Each one works differently on the inside, but LangChain gives you a **\"Universal Remote\"** to control them all.\n",
    "This \"Universal Remote\" is called the **Runnable Interface**. It means you can press the \"Play\" button (`invoke`) on *any* model, and it will work exactly the same way. You don't need to learn a new remote control just because you bought a new TV.\n",
    "\n",
    "### ðŸ§  Interface Concepts\n",
    "all Chat Models implement the **Runnable Interface**. This guarantees they all speak the same language:\n",
    "1.  **Standard Inputs:** They all accept a list of `Messages` (System, Human, AI).\n",
    "2.  **Standard Outputs:** They all return an `AIMessage`.\n",
    "3.  **Standard Methods:**\n",
    "    * `.invoke()`: Send a message and wait for the full answer.\n",
    "    * `.stream()`: Get the answer word-by-word (like a typewriter).\n",
    "    * `.batch()`: Send 50 questions at once and get 50 answers back.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `init_chat_model`: A universal function to load a model.\n",
    "* `model=\"gpt-4o-mini\"`: We are selecting a specific, fast model from OpenAI.\n",
    "* `temperature=0`: We set this to 0 to make the AI **factual and consistent**. Higher numbers (up to 1) make it more creative and random.\n",
    "\n",
    "- Supports over 100 model providers : https://docs.langchain.com/oss/python/integrations/chat\n",
    "- https://docs.langchain.com/oss/python/langchain/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain.chat_models import ChatOpenAI\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"Could not import ChatOpenAI. Install 'langchain-openai' or a recent 'langchain' package.\"\n",
    "        ) from e\n",
    "\n",
    "# Shim for the missing init_chat_model API used elsewhere in the notebook\n",
    "def init_chat_model(model: str = \"gpt-4o-mini\", temperature: float = 0, **kwargs):\n",
    "    return ChatOpenAI(model=model, temperature=temperature, **kwargs)\n",
    "\n",
    "llm = init_chat_model(model=\"gpt-4o-mini\", temperature=0) # Tip: Try other models/model providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd2aeb",
   "metadata": {},
   "source": [
    "## 3) LLM Invocation  \n",
    "\n",
    "* **Invoke:** This is like sending a text message and waiting for the reply. You don't see anything until the whole message arrives.\n",
    "* **Stream:** This is like watching someone type in real-time. You see the words appear one by one. This feels much faster to a user.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `llm.invoke(messages)`: Sends our formatted prompt to the AI and waits for the full `AIMessage` response.\n",
    "* `llm.stream(messages)`: Returns a generator that prints tokens as they arrive.\n",
    "\n",
    "ðŸ“š **[Docs](https://docs.langchain.com/oss/python/langchain/models#invocation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0a154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Tomorrow's date is 2025-12-18.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 73, 'total_tokens': 85, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_644f11dd4d', 'id': 'chatcmpl-CnieySasCdeXsx7l3BFL4yZgoF6xd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--0ce35be4-9b96-48a9-8264-eae858660b31-0', usage_metadata={'input_tokens': 73, 'output_tokens': 12, 'total_tokens': 85, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke : \n",
    "response = llm.invoke(messages)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f181e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomorrow's date is 2025-12-18.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21802a",
   "metadata": {},
   "source": [
    "## 3.1) Streaming (Real-time Output)\n",
    "\n",
    "Imagine you hired a ghostwriter to write a story.\n",
    "* **Invoke (Standard):** The writer goes into a private room, locks the door, writes the entire story, and comes out 20 minutes later to hand you the finished manuscript. You sit in silence waiting the whole time.\n",
    "* **Stream (Real-time):** The writer sits right in front of you and types. You see every letter appear on the screen the moment they press the key. The story takes the same amount of time to finish, but it **feels instant** because you are engaged immediately.\n",
    "\n",
    "### Technical Concept\n",
    "LLMs don't \"think\" of a whole sentence at once; they generate text **one token (word part) at a time**.\n",
    "* **The Bottleneck:** By default (`.invoke()`), the program buffers (holds) all these tokens in memory until the model is completely finished. This creates **Latency** (the delay between asking and seeing the answer).\n",
    "* **The Solution:** The `.stream()` method bypasses this buffer. It creates a Python **Generator** that yields each token (`AIMessageChunk`) the exact millisecond it is created. This drastically lowers the \"Time to First Token.\"\n",
    "\n",
    "### Code Context\n",
    "In the code below:\n",
    "* `llm.stream(messages)`: This replaces `.invoke()`. It returns an iterable stream, not a final string.\n",
    "* `print(..., end=\"\", flush=True)`: We force Python to print *immediately* without moving to a new line, creating that smooth \"typewriter\" effect.\n",
    "\n",
    "ðŸ“š **[Docs: Streaming](https://docs.langchain.com/oss/python/langchain/streaming)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f7300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomorrow's date is 2025-12-18."
     ]
    }
   ],
   "source": [
    "# Streaming : \n",
    "\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35d225",
   "metadata": {},
   "source": [
    "## 4) Chaining with LCEL (LangChain Expression Language)\n",
    "\n",
    "Imagine a factory assembly line:\n",
    "* **Station A:** Puts dough on the belt (The Prompt).\n",
    "* **Station B:** Bakes the dough (The Model).\n",
    "* **Station C:** Packages the bread (The Output Parser).\n",
    "\n",
    "LCEL lets us pipe (`|`) these steps together. The output of one step automatically becomes the input of the next.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `chain = prompt | llm | StrOutputParser()`\n",
    "* The `|` symbol is the magic pipe.\n",
    "* `StrOutputParser`: Converts the complex AI message object directly into a simple string so we don't have to do `print(response.content)`.\n",
    "\n",
    "ðŸ“š **[Docs: LCEL Conceptual Guide]()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda41d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomorrow's date is 2025-12-18.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "print(chain.invoke({ 'user_query' : example_user_query, \n",
    "                    'todays_date' : datetime.now().strftime(\"%Y-%m-%d\") }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb46a4c",
   "metadata": {},
   "source": [
    "## 5) Structured Output\n",
    "\n",
    "Usually, AI likes to chat and write paragraphs. But sometimes, you want a form filled out, not a conversation.\n",
    "If you ask for a date range, you don't want *\"Sure! The start date is...\"*. You want `{\"start\": \"2025-01-01\", \"end\": \"2025-01-05\"}`.\n",
    "Structured output forces the AI to stop chatting and generate data matching a specific schema.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `class DateRange(BaseModel)`: We define the \"Form\" we want the AI to fill out using Pydantic.\n",
    "* `llm.with_structured_output(DateRange)`: We tell the LLM, \"Your output **must** match this class.\"\n",
    "* The result is an actual Python object, not a string.\n",
    "\n",
    "ðŸ“š **[Docs: Structured Output](https://docs.langchain.com/oss/python/langchain/models#structured-output)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6db181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supports\n",
    "    # - pydantic\n",
    "    # - Typedict\n",
    "    # - Dataclass \n",
    "    # - Json schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2658e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateRange(start_date='2025-12-18', end_date='2025-12-21')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Optional, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_prompts import DATE_EXTRACTOR_PROMPT_TEMPLATE\n",
    "\n",
    "# You can define structure with Pydantic\n",
    "class DateRange(BaseModel):\n",
    "    start_date: str = Field(default = None, description=\"Start Date of the date range\")\n",
    "    end_date: str = Field(default = None, description=\"End Date of the date range\")\n",
    "\n",
    "# OR you can define structure with TypedDict\n",
    "# class DateRange(TypedDict):\n",
    "#     start_date: str\n",
    "#     end_date: str\n",
    "\n",
    "\n",
    "llm = init_chat_model(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "structured_llm = llm.with_structured_output(DateRange)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", DATE_EXTRACTOR_PROMPT_TEMPLATE),\n",
    "    (\"human\", \"{user_query}\")\n",
    "])\n",
    "\n",
    "\n",
    "user_query = \"I need time off for the next 4 days\"\n",
    "todays_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "llm_response = chain.invoke({'todays_date': todays_date, 'user_query': user_query})\n",
    "\n",
    "llm_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8960f",
   "metadata": {},
   "source": [
    "## 6) Tool Calling\n",
    "\n",
    "LLMs are great at writing poetry, but they are often bad at math and they don't know the current weather.\n",
    "Tool calling is like giving the AI **specialized gadgets** to help it do its job.\n",
    "You tell the AI: *\"I am giving you a calculator tool. If you see a math problem, don't guessâ€”use the tool.\"*\n",
    "The AI decides **when** it needs help and **which** gadget to pick to solve the problem.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `@tool`: This is a decorator. It puts a \"sticker\" on your Python function that says *\"Hey AI, you are allowed to use this!\"*\n",
    "* `llm.bind_tools(tools)`: This effectively hands the manual of available tools to the \"Brain.\"\n",
    "* **Important:** When the model uses a tool, it doesn't return a text answer immediately. It returns a `tool_call` (a request to run the code). You (the code) must then run that function and give the answer back to the AI.\n",
    "\n",
    "ðŸ“š **[Docs: Tool Calling](https://docs.langchain.com/oss/python/langchain/models#tool-calling)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d418671",
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.tools import tool\nfrom langchain_core.messages import HumanMessage\nimport math\n\n@tool\ndef circle_area(radius: float) -> float: \n    \"\"\"Calculate the area of a circle given its radius.\"\"\"\n    return math.pi * radius * radius\n\ntools = [circle_area]\n\n# Bind tools to the model so it can decide to call them\nllm_with_tools = llm.bind_tools(tools)\n\nmsg = HumanMessage(content=\"I have a circular garden of radius 3. What's the area?\")\nai_msg = llm_with_tools.invoke([msg])\n\nprint(\"Model Response Type:\", type(ai_msg))\nprint(\"\\nTool Calls:\", ai_msg.tool_calls if ai_msg.tool_calls else \"None\")\n\n# Demonstrate the full tool execution flow\nif ai_msg.tool_calls:\n    print(\"\\nâœ“ The model decided to use a tool!\")\n    for tool_call in ai_msg.tool_calls:\n        print(f\"  Tool: {tool_call['name']}\")\n        print(f\"  Arguments: {tool_call['args']}\")\nelse:\n    print(\"\\nâœ— The model responded directly:\", ai_msg.content)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008bbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: The area is approximately 28.27 square units.\n"
     ]
    }
   ],
   "source": [
    "# If a tool call is present, execute it and return a final answer:\n",
    "\n",
    "final = None\n",
    "if getattr(ai_msg, \"tool_calls\", None):\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        if tool_call[\"name\"] == \"circle_area\":\n",
    "            r = float(tool_call[\"args\"][\"radius\"])\n",
    "            result = circle_area.invoke({\"radius\": r})\n",
    "            final = f\"The area is approximately {result:.2f} square units.\"\n",
    "else:\n",
    "    final = ai_msg.content\n",
    "\n",
    "print(\"Final:\", final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba47ae",
   "metadata": {},
   "source": [
    "## 6.1) Tool Calling - MCP (Model Context Protocol)\n",
    "\n",
    "**Note:** MCP requires Python 3.10+. If you're using Python 3.9, skip this section - you've already learned the core concept of tool calling above!\n",
    "\n",
    "Standard tool calling (above) works great for tools defined in your own code. But what if you want to use a tool living on a different server or a database?\n",
    "**MCP** is like a **Universal USB Standard** for AI tools. It allows your LangChain app to plug into external servers to fetch tools, without you having to write the tool logic yourself.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `MultiServerMCPClient`: Connects to a local script (`mcp_server.py`) acting as a tool provider.\n",
    "* `client.get_tools()`: Automatically fetches the available tools from that server so the LLM can use them.\n",
    "\n",
    "ðŸ“š **[Docs: Model Context Protocol](https://docs.langchain.com/oss/python/langchain/mcp)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP - Model Context Protocol\n",
    "# This feature works with Python 3.10+\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient  \n",
    "\n",
    "\n",
    "client = MultiServerMCPClient(  \n",
    "    {\n",
    "        \"area\": {\n",
    "            \"transport\": \"stdio\",  # Local subprocess communication\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"./mcp_server.py\"]  # Path to your mcp_server.py file\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()  \n",
    "\n",
    "tools # You can now bind tools to the model as we did before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc2793",
   "metadata": {},
   "source": "## 7) RAG (Retrieval Augmented Generation)\n\nStandard LLMs take tests from memory (and might \"hallucinate\" if they haven't studied). **RAG** allows the AI to take an **\"Open Book\" Test**.\n\n1. **Loader:** We buy the textbook (e.g., download the Python programming language Wikipedia page).\n2. **Splitter:** We rip the pages into small paragraphs so they are easier to handle.\n3. **Vector Store:** We file these paragraphs in a cabinet organized by *meaning* (numerical vectors).\n4. **Retriever:** When you ask a question, we find the specific paragraphs relevant to that question and give them to the AI to read before answering.\n\n\n![Diagram](./images/Basic_Rag.png)\n\n###  Code Context\nIn the code below:\n* `WebBaseLoader`: A tool to scrape the text from a Wikipedia page.\n* `RecursiveCharacterTextSplitter`: Cuts the long text into chunks of 1000 characters so they fit in the model's context window.\n* `InMemoryVectorStore`: A temporary database that stores the \"meaning\" of the text chunks.\n* `rag_chain`: The final pipeline that takes your question â†’ finds docs â†’ sends both to the LLM â†’ gives you the answer.\n\nðŸ“š **Official Documentation:**\n* [Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)\n* [Text Splitters](https://python.langchain.com/docs/concepts/#text-splitters)\n* [Retrievers](https://python.langchain.com/docs/concepts/#retrievers)\n* [Vector Stores](https://python.langchain.com/docs/integrations/vectorstores/)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_prompts import RAG_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e28dc",
   "metadata": {},
   "outputs": [],
   "source": "# 1. Load the Python programming language Wikipedia page using a LangChain document loader\nprint(\"Loading Wikipedia page...\")\nloader = WebBaseLoader(\n    web_paths=(\"https://en.wikipedia.org/wiki/Python_(programming_language)\",),\n)\ndocs = loader.load()\ndocs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split docs into smaller chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447794a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['90a4d120-eb3e-42aa-9de9-facc83f783b8',\n",
       " '03d647b7-4a5a-401c-8b0f-3eab2a5d8fab',\n",
       " 'ea7c838e-82cd-4c2c-aff8-7b807290aeee',\n",
       " 'dbc27253-7bce-4f5d-aeeb-311a0e3d3f2d',\n",
       " 'd5e4e69e-74fe-4d37-b3bf-c7e73049cf38',\n",
       " 'c624f1f5-b8c2-4162-bc68-dae69fa0df27',\n",
       " 'cf847754-2608-41b4-8730-f37de68a86b4',\n",
       " '0ba18df9-ced2-4615-818a-a480f077f5a3',\n",
       " 'f526bc37-9ec3-4647-80ad-4a847b6a5539',\n",
       " '67449c8a-f533-4a9b-a389-f8b77c21b850',\n",
       " '7a945b91-0481-4774-8089-36a5c254e582',\n",
       " 'c4e393ea-e7ab-4f37-bd95-27614309bc33',\n",
       " '814a845f-c290-4420-9674-2f054ab4e959',\n",
       " '3463eaeb-8d74-41c9-95dd-5d9d537d2c10',\n",
       " '343c1717-09f7-4958-a53a-e02cb1f97b75',\n",
       " '4ffc03ea-16be-4cb0-a31a-36cb8370dc93',\n",
       " '1ff511f3-d51f-4b38-aca8-1de28085edd1',\n",
       " 'aaf77f04-3568-407d-8b36-093c686fd84c',\n",
       " 'be748929-428f-4753-8093-342b01e21b1c',\n",
       " '86c28e7b-e3b8-4b2d-acef-0016d94882c8',\n",
       " '247c7b8f-ba29-4ee8-8333-c8bd5be34510',\n",
       " 'f1824bd0-4757-4480-9dcf-6f1645de4df4',\n",
       " 'df6b4462-8ddc-48ce-8d4c-e86683435b0b',\n",
       " '5b3da9ae-5b4b-45e1-a1ff-a8e97698caa8',\n",
       " '3fdc6f5c-0d0d-4fe5-9018-49cc1ad8ef7d',\n",
       " '5d3edb67-c3f1-47c8-9cc6-f4b5a88676a8',\n",
       " '888591b2-a819-4851-94a1-f55d6f07d808',\n",
       " 'fae030aa-9752-4220-b582-3a01092fa4a9',\n",
       " 'ef151d62-8b3e-4755-b69c-c5cdd7620eeb',\n",
       " 'a86fc759-787f-4dfe-b6a5-2fc89fb198de',\n",
       " '6a3058c9-e857-4ce2-a54c-e0a2b2d3d879',\n",
       " '6dbe6061-13e0-4f3d-b8c6-a54904ca6b72',\n",
       " '99549205-119f-4db4-8672-c91504e01d92',\n",
       " '3eaa9b00-8ae7-4ec2-b42b-48cde693d97b',\n",
       " '5378ad28-f0d2-40ef-b41f-22528b5d4a67',\n",
       " '51651ab4-8e66-4d1d-876f-ef2cb0d3ceda',\n",
       " 'd4d0a16e-fc43-481a-842e-4d805fbda1de',\n",
       " '01a01a27-0761-47d8-a0c9-116c1ac50164',\n",
       " 'a273522c-fcf9-4446-8e30-de56e3e79ab7',\n",
       " '1fe47c6e-bf7b-4afe-bdc7-5adec2e2dab3',\n",
       " '6308c766-f256-4c9b-aab6-5200b54b6956',\n",
       " '8ed37ce5-c79a-4f05-9d51-72ec3288f67d',\n",
       " 'c8d04c7f-2624-4365-a08f-587d9a0969ce',\n",
       " '99ab6566-94c6-48db-bf77-5dd28150aeca',\n",
       " '3473f1c4-7b3c-466a-ad62-2e19cce84d99',\n",
       " '5949ad0d-480e-4a2d-968d-866de219d2e8',\n",
       " '43671dba-9e12-4cdc-acf9-a04dacc8b9cb',\n",
       " '27caae66-901b-4729-97fd-220f001937e4',\n",
       " '23a36f14-48b3-4e36-8b39-15542778d8e2',\n",
       " '2d768e83-612e-46aa-955e-68a00ee3618e',\n",
       " '7e3c2c55-4075-4d4c-80af-35d4c5158726',\n",
       " 'cb75b2d0-9a05-41ea-afb9-19fa457b7bfc',\n",
       " 'b36cfeb3-b16c-4003-b8e1-d734ff2c7ed9',\n",
       " '96e00193-ee3a-4430-97bf-73b7ca157176',\n",
       " '0fa90689-c285-4a15-a6ff-288b343ca34f',\n",
       " 'd6f9bbbd-8ac8-45f1-a4f3-d20e9431dc32',\n",
       " '92beebb7-96fe-43e4-8983-27e213841f16',\n",
       " 'd1c7af8c-7de2-4c71-8ae3-5324286e80f7',\n",
       " 'e3d94c1e-ecc8-4391-9e8d-3bd8f1a92c82',\n",
       " 'ade61d16-e6fe-4998-b75f-4aeadb128edd',\n",
       " '32b8ebee-818a-4b40-aa5d-bc8872790a20',\n",
       " 'd52e7288-d1f4-46a3-a8fe-31123ad330c3',\n",
       " '49758962-91f5-4e96-8d49-d95090f99582',\n",
       " '7179eabe-5acb-46eb-be21-b5cf2c33421d',\n",
       " 'dfa483d8-44d3-432d-8f28-059662cec96f',\n",
       " 'b0a8b83c-76bb-40dc-97ef-58122f1288a8',\n",
       " '4024f42f-77dd-4306-a936-65e9991dd1b2',\n",
       " '6dffb536-950c-43d7-925d-419a575402db',\n",
       " '5074286e-2456-4637-9909-dde4c864835a',\n",
       " '0367f9ee-a4e7-43fd-9145-97bf2db8eb99',\n",
       " '4d68cf2f-e14a-4007-9676-bb44577c083a',\n",
       " 'f160e16a-07fc-49d2-9a67-2890ae9418d3',\n",
       " 'd69d9361-213f-46c2-96ae-9e7b3e829560',\n",
       " 'c14852cf-28c5-4a49-b895-1e9ef42b7c5c',\n",
       " '32e5a48c-7ff2-49cc-83e2-8f8bb7eabd0d',\n",
       " '4e3fa8bb-a96d-47bf-ba65-854b8894e5b9',\n",
       " '86a2cc4f-07a5-4f5d-8593-24172e032e0f',\n",
       " 'b8a400e2-0e22-4aee-9918-5bdeaab7e638',\n",
       " 'f39cfebb-accd-42da-ae59-e05fa9665113',\n",
       " '8069d5a0-951f-4c6d-89ea-663980b0d195',\n",
       " 'ecc3fb56-558b-4549-84c7-ddb842c68861',\n",
       " '9d933415-9136-46cb-8745-38ce33c21793',\n",
       " 'a45c1f82-0af9-4c3e-a108-7f4e32392adf',\n",
       " 'f148c49d-3d4a-48c3-b8f8-9edb40f9a2f5',\n",
       " 'd8dd362b-7335-41a1-83e5-988ddb566593',\n",
       " 'c1324fd3-7df9-492f-a746-507e251d4f1d',\n",
       " '361a360d-0f4d-42dc-9072-0587f6580e95',\n",
       " '3e802da8-2476-4a1f-af4e-b82138d84701',\n",
       " 'f85c56b3-ca75-45f0-9ffc-718b3022a856',\n",
       " '3cd8a2c3-49ad-4632-90dc-b8f4f331a715',\n",
       " 'b9adaea9-d0a5-489a-84ba-fd4cbcd44469',\n",
       " '2aab4d39-4229-4620-8a51-44b2ff228034',\n",
       " 'd6ba05b5-42db-4859-a1ad-da115a4c72c6',\n",
       " '16f6083b-1689-4ead-9780-88905d676f77',\n",
       " 'caab41a5-d917-4746-8c57-bd92e004febf',\n",
       " 'e1687a42-197f-4008-b62a-c2aaed225d22',\n",
       " '0b75dec9-df05-40bc-82ff-d0255f4266c7',\n",
       " 'c642c220-ec74-40ec-84ce-af3c9411445a',\n",
       " 'bcfd48e5-e6e7-4eb9-9715-0128aa7eca71',\n",
       " '58b9cb67-a840-4cc7-a637-5021e3d5bec9',\n",
       " '76e2ca8c-fa6f-44b1-b253-7c6f4ec5b0a4',\n",
       " 'd16275c4-f074-4bd4-88c6-e7e252691e33',\n",
       " '06d38254-7fb2-4dea-97df-54c9357df85c',\n",
       " 'dd8b8b20-a961-4783-9994-6b60c712a185',\n",
       " 'bfa4d65e-f0fe-4b76-92e8-ab2538055398',\n",
       " '7af0c0aa-350a-4538-8766-702c08f4e1fc',\n",
       " 'af4f426d-3d0d-409a-a27a-93d4afdda8ad',\n",
       " '09e02623-e0a4-4622-8ef8-d8206051f458',\n",
       " '147917fd-f4a0-4ace-b712-666e6df98844',\n",
       " '942a4319-617a-4e7b-976c-7697be3f6bd8',\n",
       " '59a2589e-d844-4e77-ab74-50b57d6f0e8a',\n",
       " '1edd7c64-2088-4d7b-8584-f6ef308025bf',\n",
       " '31f08a79-1c57-4954-9261-a554582fef1f',\n",
       " 'a416fa4c-0566-4ff2-bd89-b1c13769cb3c',\n",
       " '3ca1902c-a103-4ee5-a79c-6682573d5c1b',\n",
       " 'c490b681-6892-464a-9d8c-925d624794d8',\n",
       " 'c3d4260e-1131-4d82-9f6f-ae241c05a628',\n",
       " 'cb5dd99e-c955-441d-b5cd-6d1e0372f24a',\n",
       " '260fc67d-efa7-45d5-b45b-59389bc068a8',\n",
       " '9ddb5ca1-a070-4ec7-abb0-8519b99dccd8',\n",
       " '4ea5f852-5992-41ec-b3c0-b99fa50f4cd5',\n",
       " 'ca3a53e1-59f5-4276-b0ac-f55eb746a0af',\n",
       " 'e9eaa8bc-ffc7-4f09-aede-f71de5f70c03',\n",
       " '3e8da3ae-2e74-4888-aeb6-65d117243c45',\n",
       " 'bc9401c9-8ba3-4a1d-9156-7e713ad04dcb',\n",
       " 'a14afb74-8881-410e-a17e-470c9322427e',\n",
       " 'c197cd9e-4397-461a-8051-dcbb674cfb73',\n",
       " '72573ff7-b92e-4167-be01-638a3e46d45b',\n",
       " '4ba49aa8-39b6-4adc-91b4-34132ad0f8b9',\n",
       " '61947691-8b64-4d1e-8700-ff9ead342c1b',\n",
       " '84dc14e8-7798-4a04-83a6-32d3be70dd43',\n",
       " '229b10b9-e401-422d-b4e3-3a63d80d1d67',\n",
       " '7a91ff61-b909-4c73-90fc-6f57ada2dcd3',\n",
       " 'ce10bf65-a80c-47e8-bbc3-6ef73c48385f',\n",
       " '26bb0d58-31f3-4f48-97d5-96199c627f24',\n",
       " '4cdb9451-7761-46c9-8940-73ab31c4f39b',\n",
       " 'bd889f5c-6698-4af2-bd87-1104e0589e92',\n",
       " '712ffa2a-5719-444b-aa3b-3265744f2c7f',\n",
       " '3d9e1acc-991c-4b87-b798-0a582016f71d',\n",
       " '3b2868bc-844a-44be-b4ec-ebd0fc6fa30d',\n",
       " '66597834-a976-4282-8341-6872e3cf8fa5',\n",
       " 'd1b73187-7c45-4eb1-9bd2-c22932490d80',\n",
       " '260f7992-82fa-49bd-8145-80a8258bcd27',\n",
       " '6252c649-089a-47fe-81ab-bdcddbd6116f',\n",
       " '34138a29-3d4a-4e1d-8ccc-54318b120687',\n",
       " '5a27277f-42ed-401b-8493-dc92e61f0a17',\n",
       " 'd31a63bb-f388-4aa4-8c44-83f4135a64d9',\n",
       " '21cef666-0f10-4669-86d6-817601b98ab9',\n",
       " '410c8503-86e7-46d6-bc1e-f6a33aa1acdd',\n",
       " 'a563ee66-1e0e-4133-bbea-db518252c32a',\n",
       " 'b922515e-67e2-42d4-8a93-3ab77c1ade92',\n",
       " '3c83650c-a115-436c-b9b3-0a888977a643',\n",
       " '5f7ea705-3add-4a65-b086-31e398f0fae5',\n",
       " 'bd30708e-a846-49e3-9d5f-db3e60c0f677',\n",
       " '3d0b7e51-3489-473c-a841-ce7ebf799585',\n",
       " '7864aefe-41a5-4cd0-8bd9-82740530b579',\n",
       " '7bbd6481-f53a-4b2e-b6d4-3f7732480847',\n",
       " '08a808d6-be6b-4c8d-8380-e7fbfbdac96d',\n",
       " 'd8e16697-2224-475b-ab86-336f1c5329fe',\n",
       " '80593263-f285-4bcf-a4cb-713186466243',\n",
       " '8fb6afc0-6f5e-42e2-97d3-810ed3ee0815',\n",
       " 'e6ab482e-9510-44ea-8353-fb927cfd8d77',\n",
       " 'c5ba89f0-4b68-474d-8940-c4930f26e7b2',\n",
       " 'b3dcd32c-f492-4e19-b400-02c91dd70346',\n",
       " 'ff2e726e-9ee9-406d-ab0d-685236315765',\n",
       " 'ca1eb814-beaa-4866-95b0-b972f4de4195',\n",
       " '3be981b2-a337-4174-98db-919b2fae32fd',\n",
       " 'f4e81eea-80d5-4041-ab9f-844bda3c6d54',\n",
       " '43214147-ae30-4e26-9c82-74a207966bbe',\n",
       " '216b28d1-a548-4497-bcac-232945ac0453',\n",
       " '5ed7a927-6303-445b-877d-dede01d0ba26',\n",
       " '3f315641-83f0-4f22-b2ec-3afd6b12e250',\n",
       " 'e7d9332e-e316-448b-8045-20b4fe8a6447',\n",
       " '7e18d96f-2e34-4a1d-af92-0ca9a493800f',\n",
       " 'f4a56843-2a94-429a-8b54-74729b08776d',\n",
       " '345a915e-cbea-484a-b5b6-5f949601b4cc',\n",
       " '9efaf7f6-373c-47f2-a6af-b79a43c1e2a5',\n",
       " '5285f904-8ebe-4319-b286-a8a8c38900cb',\n",
       " 'ae15bd1f-f466-4a61-a627-ecd09666fdd8',\n",
       " '510e418d-daf8-4118-9c0d-ed8f6bfcb2c0',\n",
       " 'e8fb0774-6b02-44f1-9a53-8f005ede6a1d',\n",
       " 'b70dbeec-15ff-4a1d-820c-43deb9ed8652',\n",
       " 'be05ab72-4ed8-4a1e-a48a-4d3a71894b0a',\n",
       " 'c24bd462-e7f8-4d8f-95fa-3ab4c2988c32',\n",
       " '05b579a6-a4c8-4e95-bdd9-82e928cb366f',\n",
       " '2d2f40e0-c55a-4c87-9e9c-06a400031577',\n",
       " '9928aa1f-bd0d-48df-9ba3-3d627a923fca',\n",
       " '337fa609-8ffa-4694-9ed1-65e18c540c20',\n",
       " 'ae00f47c-493d-40fc-9781-af5922afcc31',\n",
       " 'd5afdffd-b2c0-4b5e-b28b-dadf894a2d36',\n",
       " '112db1d9-0754-472c-8fec-f63e1b7263f4',\n",
       " 'b2f89331-b8aa-42d8-a1ab-be8e226e0dc0',\n",
       " 'ebc1961b-20f4-403a-9ef2-16e23cd48e2e',\n",
       " '21ae7ded-2714-4310-94c6-24def3b215ce',\n",
       " '23fcbba1-282a-4294-86ef-05f8c6e48c17',\n",
       " '959a2d11-ab20-4d94-ba88-5b1466293905',\n",
       " '257faad1-2338-4cc6-8cb7-212659ba1565',\n",
       " '8ef90a16-489d-4a29-98b4-fbe4404d23c5',\n",
       " 'aa49ea62-eccc-4571-8ff5-e586e6991a16',\n",
       " 'bfbdcc4e-51bd-4641-af17-9b64e76397ba',\n",
       " '8337a2ae-f9f4-4984-bfd2-77aa90ec13dc',\n",
       " '703439d0-1e78-4e82-8b85-1098f6f07146',\n",
       " '1c6323ee-3121-4c23-846d-3a1088744dc0',\n",
       " '068015dc-620d-47aa-a86e-8d45273afd95']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Create an in-memory vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# Add the chunks to the in-memory store\n",
    "vector_store.add_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define a retreiven for the vector store (top-k)\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    "    )\n",
    "\n",
    "# Helper to join document contents into a single string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6696f",
   "metadata": {},
   "outputs": [],
   "source": "user_query = \"Who created Python and when was it first released?\"\nretriever.invoke(user_query)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df304784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RAG Q/A Chain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(format_docs),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cca5ca",
   "metadata": {},
   "outputs": [],
   "source": "user_query = \"Who created Python and when was it first released?\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef919fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SpaceX was founded on March 14, 2002, in El Segundo, California, U.S. The company was established by Elon Musk. Since its founding, SpaceX has made significant advances in rocket propulsion, reusable launch vehicles, human spaceflight, and satellite constellation technology.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = rag_chain.invoke(user_query)\n",
    "result.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9b425",
   "metadata": {},
   "source": [
    "### ðŸ“š Recommended Resources\n",
    "* **[LangChain Python Tutorials](https://python.langchain.com/docs/tutorials/)**: The official step-by-step guides.\n",
    "* **[LangSmith](https://smith.langchain.com/)**: A tool to trace and debug your chains (vital for production).\n",
    "* **[LangGraph Documentation](https://langchain-ai.github.io/langgraph/)**: The future of building complex agents.\n",
    "* **[LangChain YouTube Channel](https://www.youtube.com/@LangChain)**: Great for visual learners and deep dives into specific topics.\n",
    "* **[Usecases](https://docs.langchain.com/oss/python/learn)**: Explore real-world usecases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}